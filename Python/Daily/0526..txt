가위바위보 한 이유 : 개와 고양이 이진분류가 아닌 다중분류를 해보기 위해

dropout
early_stopping

상관계수 0.5미만은 버려도 된다


일반적인 데이터는 읽어서 결측치 없에고 이상치 없에고

상관계수 보고 필요없는거 지우고

표준화 하고

정규화 하고 나눈다


input output hidden 레이어 어떻게 구성하는지 외우고

평가하고 예측한다

마지막으로 accuracy 와 loss 의 그래프를 그려준다

완만하게 진행이 됬으면 잘된것

acc는 85%넘는것만 업계에서 인정을 해줌


과적합시 레이어를 줄이거나 dropout으로 노드를 줄인다


Decision Tree
특징 기준에 따라 데이터를 구분하는 모델 한번의 분기 때마다 변수 영역을 두개로 구분함
고객 타게팅 시장 세분화 가격 예측등에 사용함

Random Forest
의사결정 트리의 한 종류 결정 트리를 여러개 만들어 그 결과를 종합하는 앙상블 기법 모든 트리의 각 선택을 고려하여 가장많이 선택된 분류를 고름
모션 캡처 주변기기인 키넥트에서 랜덤포레스트를 이용하여 주어진 입력에서 신체 각 부분을 분류 다채널 자기공명영상등에 사용함

Gaussian Naive Bayes
베이즈 정리에서의 확률 기반 각 속성이 서로 독립적인 경우에만 적용 가능
많은 클래스가 필요할때 주로 사용

KNN
거리를 기반으로 데이터를 가장 가까운 유사 속성에 따라 분류 데이터로 부터 거리가 가까운 k개의 \다른 데이터를 참조하여 분류 예측

AdaBoost
부스팅 알고리즘의 일종
예측 성능이 낮은 약한 학습기를 다량구축및 조합하여 가중치 수정을 통해 분류

이차 판별 분석
결정 경계선이 비선형이여서 서로 다른 공분산 구조를 갖는 데이터에 적용

SVM
각 데이터의 점들을 선을 사용해서 구분함 이선은 데이터 범주를 구분지을수 있는 가장 가까운 2개 데이터의 점을 기준으로 생성

Voting
다른 종류 모델들의 에측 값을 합쳐서 최종 결과를 도출해냄

Bagging
동일 알고리즘의 약한 학습기를 여러 개 사용해서 각각의 boostrap sample


RNN은 시계열 데이터를 처리하기 위해

하지만 RNN은 기울기를 잃어버린다 (기억을 잃는다)

그래서 나온게 LSTM

일대일, 일대다, 다대일, 다대다, 동기화 다대다

결과값과 정답을 비교후 w, b를 업데이트 하는것을 역전파를 이용 미분하여 업데이트


RNN은 가중치가 업데이트 되는 과정에서 기울기가 1보다 작은 값이 계속 곱해지기 때문에
기울기가 사라지는 기울기 소멸 문제가 발생

이를 해결하기 위해 LSTM이나 GRU같은 확장된 RNN 방식들을 사용하고 있음

기억할것(계산한 값이1에 가까운것)은 계속 기억하고
잊어버릴(계산한 값이0에 가까운)것은 빨리 잊어버리는 방식

망각 게이트
과거

입력 게이트
현재

출력 게이트
미래

LSTM 계층

units:네트워크의 층 수
dropout:전체 가중치 중 50% 값을 0으로 설정하여 사용하지 않겠다는 의미
return_sequences=False는 마지막 셀에서 밀집층이 한번만 적용되었다는 것을 의미
unroll 시간순서에 따라 입력층가 은닉층

오후는 자연어 처리


월요일 powerBi


자연어

토큰화

불용어 제거

어간 추출

정 규화

단어 벡터 변환

결정 트리

서포트 벡터 머신


Gensim



